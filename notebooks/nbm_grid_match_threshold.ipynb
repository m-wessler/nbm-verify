{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import pygrib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import multiprocessing as mp\n",
    "\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "nbm_dir = '/scratch/general/lustre/u1070830/nbm/'\n",
    "urma_dir = '/scratch/general/lustre/u1070830/urma/'\n",
    "nbm_shape = (1051, 1132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_fhr(nbm_file, xthreshold, xinterval, returned=False):\n",
    "    \n",
    "    # WE NEED TO MATCH URMA IN HERE IF WE CAN! \n",
    "    \n",
    "    print(nbm_file.split('/')[-2:])\n",
    "    \n",
    "    with pygrib.open(nbm_file) as grb:\n",
    "        \n",
    "        msgs = grb.read()\n",
    "        if len(msgs) > 0:\n",
    "                    \n",
    "            _init = nbm_file.split('/')[-2:]\n",
    "            init = datetime.strptime(\n",
    "                _init[0] + _init[1].split('.')[1][1:-1], \n",
    "                '%Y%m%d%H')\n",
    "\n",
    "            if init.hour % 6 != 0:\n",
    "                init -= timedelta(hours=1)\n",
    "\n",
    "            lats, lons = grb.message(1).latlons()\n",
    "\n",
    "            valid = datetime.strptime(\n",
    "                str(msgs[0].validityDate) + '%02d'%msgs[0].validityTime, \n",
    "                '%Y%m%d%H%M')\n",
    "            \n",
    "            step = valid - init\n",
    "            lead = int(step.days*24 + step.seconds/3600)\n",
    "\n",
    "            for msg in msgs:\n",
    "\n",
    "                if 'Probability of event above upper limit' in str(msg):\n",
    "\n",
    "                    interval = msg['stepRange'].split('-')\n",
    "                    interval = int(interval[1]) - int(interval[0])\n",
    "\n",
    "                    threshold = msg.upperLimit\n",
    "\n",
    "                    if ((threshold == xthreshold)&(interval == xinterval)):\n",
    "                        \n",
    "                        returned = True\n",
    "                        return (init, valid, lead, msg.values)\n",
    "                    \n",
    "            if not returned:\n",
    "                return(init, valid, lead, np.full(nbm_shape, fill_value=np.nan))\n",
    "            \n",
    "        else:\n",
    "            print('%s: No grib messages'%nbm_file.split('/')[-2:])\n",
    "            \n",
    "    gc.collect()\n",
    "\n",
    "unpack_fhr_mp(nbm_flist_agg[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20200518', 'blend.t01z.qmd.f077.WR.grib2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2020, 5, 18, 0, 0),\n",
       " datetime.datetime(2020, 5, 21, 6, 0),\n",
       " 78,\n",
       " array([[ 1.,  1.,  1., ..., 10., 10., 10.],\n",
       "        [ 1.,  1.,  1., ..., 10., 10., 10.],\n",
       "        [ 1.,  1.,  1., ..., 11., 10., 10.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 27., 27., 27.],\n",
       "        [ 0.,  0.,  0., ..., 27., 27., 27.],\n",
       "        [ 0.,  0.,  0., ..., 27., 27., 28.]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass data label to the extractor to pull out the variable we care about\n",
    "# Do these one at a time and save out the xarray to netcdf to compare w/ URMA\n",
    "extract_threshold = 0.254\n",
    "extract_interval = 24\n",
    "data_label = 'probx_%s_%02dh'%(str(extract_threshold).replace('.', 'p'), extract_interval)\n",
    "\n",
    "# Build a list of inits\n",
    "inits = pd.date_range(\n",
    "    datetime(2020, 5, 18, 0), \n",
    "    datetime(2020, 10, 26, 23), \n",
    "    freq='6H')\n",
    "\n",
    "nbm_flist_agg = []\n",
    "for init in inits:\n",
    "\n",
    "    try:\n",
    "        nbm_flist = sorted(glob(nbm_dir + init.strftime('%Y%m%d') + '/*t%02dz*'%init.hour))\n",
    "        nbm_flist[0]\n",
    "\n",
    "    except:\n",
    "        nbm_flist = sorted(glob(nbm_dir + init.strftime('%Y%m%d') + '/*t%02dz*'%(init+timedelta(hours=1)).hour))\n",
    "\n",
    "    nbm_flist = [f for f in nbm_flist if 'idx' not in f]\n",
    "\n",
    "    if len(nbm_flist) > 0:\n",
    "        nbm_flist_agg.append(nbm_flist)\n",
    "\n",
    "nbm_flist_agg = np.hstack(nbm_flist_agg)\n",
    "\n",
    "unpack_fhr_mp = partial(unpack_fhr, xinterval=extract_interval, xthreshold=extract_threshold)\n",
    "\n",
    "# # 128 workers ~ 100GB RAM\n",
    "# workers = 128\n",
    "# with mp.get_context('fork').Pool(workers) as p:\n",
    "#     returns = p.map(unpack_fhr_mp, nbm_flist_agg, chunksize=1)\n",
    "#     p.close()\n",
    "#     p.join()\n",
    "\n",
    "# Get the latlon grid from any NBM file here:\n",
    "# sample_file = nbm_flist[0]\n",
    "# pygrib.open(sample_file).read().message(1).latlons()\n",
    "# lat, lon = latlons[0], latlons[1]\n",
    "\n",
    "# returns = np.array([r for r in returns if r is not None], dtype=object)\n",
    "# init = returns[:, 0].astype(np.datetime64)\n",
    "# valid = returns[:, 1].astype(np.datetime64).reshape(len(np.unique(init)), -1)\n",
    "# lead = returns[:, 2].astype(np.int16).reshape(len(np.unique(init)), -1)\n",
    "# data = np.array([r for r in returns[:, 3]], dtype=np.int8).reshape(len(np.unique(init)), -1, nbm_shape[0], nbm_shape[1])\n",
    "\n",
    "# valid = xr.DataArray(valid, name='valid', dims=('init', 'lead'), coords={'init':np.unique(init), 'lead':np.unique(lead)})\n",
    "# data = xr.DataArray(data, name='data_label', dims=('init', 'lead', 'y', 'x'), coords={'init':np.unique(init), 'lead':np.unique(lead)})\n",
    "# data = xr.merge([data, valid])\n",
    "    \n",
    "# Work on adding URMA\n",
    "# Write separate URMA read function, export data grids together\n",
    "# ONLY NEED TO READ URMA BASED ON VALID_TIME!!! NO NEED TO MAKE IT LARGER THAN IT NEEDS TO BE\n",
    "\n",
    "# outfile = './' + data_label + '.%s_%s.WR.nc'%(inits[0].strftime('%Y%m%d%H'), inits[-1].strftime('%Y%m%d%H'))\n",
    "# data.to_netcdf(outfile)\n",
    "\n",
    "# print('Done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
