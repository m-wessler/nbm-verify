{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in Google CoLab! (Open in new window or new tab)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m-wessler/nbm-verify/blob/master/notebooks/verify_1Dqpf_dev.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../scripts/')\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import nbm_funcs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import seaborn as sns\n",
    "import scipy.stats as scipy\n",
    "import urllib.request as req\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Configuration\n",
    "Select 'site' to evaluate, modify 'vsite' if an alternate verification site is preferred<br>\n",
    "Fixed 'date0' at the start of the NBM v3.2 period (2/20/2020)<br>\n",
    "Full lead time is 263 hours - Note if date1 is within this period, there will be missing verification data as it does not exist yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBM 1D Viewer Site to use\n",
    "# site = sys.argv[1]\n",
    "site = nbm_funcs._site = 'KSEA'\n",
    "vsite = site\n",
    "\n",
    "# Data Range\n",
    "lead_time_end = 263\n",
    "init_hours = [13]#[1, 7, 13, 19]\n",
    "\n",
    "date0 = nbm_funcs._date0 = datetime(2020, 3, 1)\n",
    "date1 = nbm_funcs._date1 = datetime(2020, 7, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitepath = site if site == vsite else '_'.join([site, vsite])\n",
    "\n",
    "datadir = nbm_funcs._datadir = '../archive/%s/data/'%sitepath\n",
    "os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "figdir = nbm_funcs._figdir = '../archive//%s/figures/'%sitepath\n",
    "os.makedirs(figdir, exist_ok=True)\n",
    "\n",
    "dates = pd.date_range(date0, date1, freq='1D')\n",
    "date2 = nbm_funcs._date2 = date1 + timedelta(hours=lead_time_end)\n",
    "\n",
    "print(('\\nForecast Site: {}\\nVerif Site: {}\\nInit Hours: '+\n",
    "      '{}\\nFirst Init: {}\\nLast Init: {}\\nLast Verif: {}').format(\n",
    "    site, vsite, init_hours, date0, date1, date2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Obtain observation data from SynopticLabs (MesoWest) API\n",
    "These are quality-controlled precipitation observations with adjustable accumulation periods<br>\n",
    "See more at: https://developers.synopticdata.com/mesonet/v2/stations/precipitation/\n",
    "<br><br>\n",
    "If no observation file exists, will download and save for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obfile = datadir + '%s_obs_%s_%s.pd'%(site, date0.strftime('%Y%m%d'), date1.strftime('%Y%m%d'))\n",
    "\n",
    "if os.path.isfile(obfile):\n",
    "    # Load file\n",
    "    obs = pd.read_pickle(obfile)\n",
    "    print('\\nLoaded obs from file %s\\n'%obfile)\n",
    "\n",
    "else:\n",
    "    # Get and save file\n",
    "    obs = nbm_funcs.get_precip_obs(vsite, date0, date2)\n",
    "    obs = obs[0].merge(obs[1], how='inner', on='ValidTime').merge(obs[2], how='inner', on='ValidTime')\n",
    "    obs = obs[[k for k in obs.keys() if 'precip' in k]].sort_index()\n",
    "\n",
    "    obs.to_pickle(obfile)\n",
    "    print('\\nSaved obs to file %s\\n'%obfile)\n",
    "    \n",
    "mm_in = 1/25.4\n",
    "obs *= mm_in\n",
    "[obs.rename(columns={k:k.replace('mm', 'in')}, inplace=True) for k in obs.keys()]\n",
    "\n",
    "obs.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Obtain NBM forecast data from NBM 1D Viewer (csv file API)\n",
    "These are the NBM 1D output files extracted from the viewer with 3 set accumulation periods<br>\n",
    "See more at: https://hwp-viz.gsd.esrl.noaa.gov/wave1d/?location=KSLC&col=2&hgt=1&obs=true&fontsize=1&selectedgroup=Default\n",
    "<br><br>\n",
    "If no forecast file exists, will download and save for future use. This can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbmfile = datadir + '%s_nbm_%s_%s.pd'%(site, date0.strftime('%Y%m%d'), date1.strftime('%Y%m%d'))\n",
    "\n",
    "if os.path.isfile(nbmfile):\n",
    "    # Load file\n",
    "    nbm = pd.read_pickle(nbmfile)\n",
    "    print('Loaded NBM from file %s'%nbmfile)\n",
    "\n",
    "else:\n",
    "    url_list = []\n",
    "    for date in dates:\n",
    "        for init_hour in init_hours:\n",
    "            # For now pull from the csv generator\n",
    "            # Best to get API access or store locally later\n",
    "            base = 'https://hwp-viz.gsd.esrl.noaa.gov/wave1d/data/archive/'\n",
    "            datestr = '{:04d}/{:02d}/{:02d}'.format(date.year, date.month, date.day)\n",
    "            sitestr = '/NBM/{:02d}/{:s}.csv'.format(init_hour, site)\n",
    "            url_list.append([date, init_hour, base + datestr + sitestr])\n",
    "\n",
    "    # Try multiprocessing this for speed?\n",
    "    nbm = np.array([nbm_funcs.get_1d_csv(url, this=i+1, total=len(url_list)) for i, url in enumerate(url_list)])\n",
    "    nbm = np.array([line for line in nbm if line is not None])\n",
    "\n",
    "    header = nbm[0, 0]\n",
    "    \n",
    "    # This drops days with incomplete collections. There may be some use\n",
    "    # to keeping this data, can fix in the future if need be\n",
    "    # May also want to make the 100 value flexible!\n",
    "    nbm = np.array([np.array(line[1]) for line in nbm if len(line[1]) == 100])\n",
    "\n",
    "    nbm = nbm.reshape(-1, nbm.shape[-1])\n",
    "    nbm[np.where(nbm == '')] = np.nan\n",
    "\n",
    "    # Aggregate to a clean dataframe\n",
    "    nbm = pd.DataFrame(nbm, columns=header).set_index(\n",
    "        ['InitTime', 'ValidTime']).sort_index()\n",
    "\n",
    "    # Drop last column (misc metadata?)\n",
    "    nbm = nbm.iloc[:, :-2].astype(float)\n",
    "    header = nbm.columns\n",
    "\n",
    "    # variables = np.unique([k.split('_')[0] for k in header])\n",
    "    # levels = np.unique([k.split('_')[1] for k in header])\n",
    "\n",
    "    init =  nbm.index.get_level_values(0)\n",
    "    valid = nbm.index.get_level_values(1)\n",
    "\n",
    "    # Note the 1h 'fudge factor' in the lead time here\n",
    "    lead = pd.DataFrame(\n",
    "        np.transpose([init, valid, ((valid - init).values/3600/1e9).astype(int)+1]), \n",
    "        columns=['InitTime', 'ValidTime', 'LeadTime']).set_index(['InitTime', 'ValidTime'])\n",
    "\n",
    "    nbm.insert(0, 'LeadTime', lead)\n",
    "\n",
    "    klist = np.array([k for k in np.unique([k for k in list(nbm.keys())]) if ('APCP' in k)&('1hr' not in k)])\n",
    "    klist = klist[np.argsort(klist)]\n",
    "    klist = np.append('LeadTime', klist)\n",
    "    nbm = nbm.loc[:, klist]\n",
    "    \n",
    "    # Nix values where lead time shorter than acc interval\n",
    "    for k in nbm.keys():\n",
    "        if 'APCP24hr' in k:\n",
    "            nbm[k][nbm['LeadTime'] < 24] = np.nan\n",
    "        elif 'APCP12hr' in k:\n",
    "            nbm[k][nbm['LeadTime'] < 12] = np.nan\n",
    "        elif 'APCP6hr' in k:\n",
    "            nbm[k][nbm['LeadTime'] < 6] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    nbm.to_pickle(nbmfile)\n",
    "    print('\\nSaved NBM to file %s'%obfile)\n",
    "\n",
    "# Convert mm to in\n",
    "nbm = pd.DataFrame([nbm['LeadTime']] + [nbm[k] * mm_in for k in nbm.keys() if 'LeadTime' not in k]).T\n",
    "\n",
    "# Display some basic stats\n",
    "nbm.loc[:, ['APCP6hr_surface', 'APCP6hr_surface_70% level', 'APCP6hr_surface_50% level',\n",
    "            'APCP12hr_surface', 'APCP12hr_surface_70% level', 'APCP12hr_surface_50% level',\n",
    "            'APCP24hr_surface', 'APCP24hr_surface_70% level', 'APCP24hr_surface_50% level'\n",
    "            ]].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the distribution of precipitation observations vs forecasts for assessment of representativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_id = nbm_funcs._thresh_id = {'Small':[0, 1], 'Medium':[1, 2], 'Large':[2, 3], 'All':[0, 3]}\n",
    "\n",
    "# 33rd, 67th percentile determined above\n",
    "thresholds = nbm_funcs._thresholds = {interval:nbm_funcs.apcp_dist_plot(obs, nbm, interval) \n",
    "              for interval in [6, 12, 24]}\n",
    "\n",
    "# Use fixed override if desired\n",
    "# thresholds = {\n",
    "#     6:[1, 2],\n",
    "#     12:[1, 2],\n",
    "#     24:[1, 2]}\n",
    "\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Reorganize the data for analysis:\n",
    "#### Isolate the forecasts by accumulation interval and lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = np.arange(1, 100)\n",
    "\n",
    "data = []\n",
    "for interval in [6, 12, 24]:\n",
    "    \n",
    "    pkeys = np.array([k for k in nbm.keys() if '%dhr_'%interval in k])\n",
    "    pkeys = np.array([k for k in pkeys if '%' in k])\n",
    "    pkeys = pkeys[np.argsort([int(k.split('_')[-1].split('%')[0]) for k in pkeys])]\n",
    "    \n",
    "    for lead_time in np.arange(interval, lead_time_end, 6):\n",
    "        \n",
    "        for esize in ['Small', 'Medium', 'Large']:\n",
    "            \n",
    "            thresh = [thresholds[interval][thresh_id[esize][0]], \n",
    "                      thresholds[interval][thresh_id[esize][1]]]\n",
    "        \n",
    "            print('\\rProcessing interval %d lead %dh'%(interval, lead_time), end='')\n",
    "\n",
    "            # We need to break out the verification to each lead time,\n",
    "            # but within each lead time we have a number of valid times.\n",
    "            # At each lead time, valid time, isolate the forecast verification\n",
    "\n",
    "            # Combine the datasets to make it easier to work with\n",
    "            idata = nbm[nbm['LeadTime'] == lead_time].merge(obs, on='ValidTime').drop(columns='LeadTime')\n",
    "\n",
    "            # Subset for event size\n",
    "            iobs = idata['%dh_precip_in'%interval]\n",
    "            idata = idata[((iobs >= thresh[0]) & (iobs < thresh[1]))]\n",
    "\n",
    "            for itime in idata.index:\n",
    "\n",
    "                try:\n",
    "                    prob_fx = idata.loc[itime, pkeys].values\n",
    "                    mean_fx = np.nanmean(prob_fx)\n",
    "                    std_fx = np.nanstd(prob_fx)\n",
    "                    med_fx = idata.loc[itime, 'APCP%dhr_surface_50%% level'%interval]\n",
    "                    det_fx = idata.loc[itime, 'APCP%dhr_surface'%interval]\n",
    "\n",
    "                    # Optional - leave as nan?\n",
    "                    det_fx = det_fx if ~np.isnan(det_fx) else 0.\n",
    "\n",
    "                    verif_ob = idata.loc[itime, '%dh_precip_in'%interval]\n",
    "                    \n",
    "                    verif_rank = np.searchsorted(prob_fx, verif_ob, 'right')                    \n",
    "                    verif_rank_val = prob_fx[verif_rank-1]\n",
    "                    verif_rank_error = verif_rank_val - verif_ob\n",
    "                    \n",
    "                    verif_rank = 101 if ((verif_rank >= 99) & (verif_ob > verif_rank_val)) else verif_rank\n",
    "                    verif_rank = -1 if ((verif_rank <= 1) & (verif_ob < verif_rank_val)) else verif_rank\n",
    "                    \n",
    "                    det_rank = np.searchsorted(prob_fx, det_fx, 'right')\n",
    "                    det_error = det_fx - verif_ob\n",
    "\n",
    "                except:\n",
    "                    raise\n",
    "                    # pass\n",
    "                    # print('failed', itime)\n",
    "\n",
    "                else:\n",
    "                    if ((verif_ob > 0.) & ~np.isnan(verif_rank_val)):\n",
    "\n",
    "                        data.append([\n",
    "                            # Indexers\n",
    "                            interval, lead_time, itime, esize,\n",
    "\n",
    "                            # Verification and deterministic\n",
    "                            verif_ob, det_fx, det_rank, det_error,\n",
    "\n",
    "                            # Probabilistic\n",
    "                            verif_rank, verif_rank_val, verif_rank_error, \n",
    "                            med_fx, mean_fx, std_fx])\n",
    "\n",
    "data = pd.DataFrame(data, columns=['Interval', 'LeadTime', 'ValidTime', 'EventSize',\n",
    "                'verif_ob', 'det_fx', 'det_rank', 'det_error',\n",
    "                'verif_rank', 'verif_rank_val', 'verif_rank_error', \n",
    "                'med_fx', 'mean_fx', 'std_fx'])\n",
    "\n",
    "print('\\n\\nAvailable keys:\\n\\t\\t{}\\nn rows: {}'.format('\\n\\t\\t'.join(data.keys()), len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Create Bulk Temporal Stats Plots\n",
    "#### Reliability diagrams, bias over time, rank over time, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot histograms of percentile rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short, long = 0, 120\n",
    "plot_type = 'Verification'\n",
    "plot_var = 'verif_rank'\n",
    "esize = 'All'\n",
    "\n",
    "for interval in [6, 12, 24]:\n",
    "\n",
    "    kwargs = {'_interval':interval, '_esize':esize,\n",
    "             '_short':short, '_long':long,\n",
    "             '_plot_type':plot_type, '_plot_var':plot_var}\n",
    "    \n",
    "    nbm_funcs.histograms_verif_rank(data, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a reliability diagram style CDF to evaluate percentile rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short, long = 0, 120\n",
    "plot_type = 'Verification'\n",
    "plot_var = 'verif_rank'\n",
    "esize = 'All'\n",
    "\n",
    "for interval in [6, 12, 24]:\n",
    "\n",
    "    kwargs = {'_interval':interval, '_esize':esize,\n",
    "             '_short':short, '_long':long,\n",
    "             '_plot_type':plot_type, '_plot_var':plot_var}\n",
    "\n",
    "    nbm_funcs.reliability_verif_cdf(data, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce bias, ME, MAE, and percentile rank plots as they evolve over time\n",
    "This helps illustrate at what leads a dry/wet bias may exist and how severe it may be<br>\n",
    "Adds value in interpreting the CDF reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short, long = 0, 120\n",
    "esize = 'All'\n",
    "\n",
    "for interval in [6, 12, 24]:\n",
    "\n",
    "    kwargs = {'_interval':interval, '_esize':esize,\n",
    "             '_short':short, '_long':long}\n",
    "\n",
    "    nbm_funcs.rank_over_leadtime(data, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
