{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import requests\n",
    "import nbm_funcs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import multiprocessing as mp\n",
    "\n",
    "import seaborn as sns\n",
    "import scipy.stats as scipy\n",
    "import urllib.request as req\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Configuration\n",
    "Select 'site' to evaluate, modify 'vsite' if an alternate verification site is preferred<br>\n",
    "Fixed 'date0' at the start of the NBM v3.2 period (2/20/2020)<br>\n",
    "Full lead time is 263 hours - Note if date1 is within this period, there will be missing verification data as it does not exist yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBM 1D Viewer Site to use\n",
    "site = nbm_funcs._site = 'KMSO'\n",
    "\n",
    "# Data Range\n",
    "lead_time_end = 263\n",
    "init_hours = nbm_funcs._init_hours = [13]#[1, 7, 13, 19]\n",
    "\n",
    "date0 = nbm_funcs._date0 = datetime(2020, 3, 1)#3, 1)\n",
    "date1 = nbm_funcs._date1 = datetime(2020, 8, 15)#today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datadir = nbm_funcs._datadir = '../archive/%s/data/'%site\n",
    "datadir = nbm_funcs._datadir = '../archive/data/'\n",
    "os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "dates = nbm_funcs._dates = pd.date_range(date0, date1, freq='1D')\n",
    "date2 = nbm_funcs._date2 = date1 + timedelta(hours=lead_time_end)\n",
    "\n",
    "print(('\\nForecast Site: {}\\nInit Hours: '+\n",
    "      '{}\\nFirst Init: {}\\nLast Init: {}\\nLast Verif: {}').format(\n",
    "    site, init_hours, date0, date1, date2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Obtain observation data from SynopticLabs (MesoWest) API\n",
    "These are quality-controlled precipitation observations with adjustable accumulation periods<br>\n",
    "See more at: https://developers.synopticdata.com/mesonet/v2/stations/precipitation/\n",
    "<br><br>\n",
    "If no observation file exists, will download and save for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata for the select point\n",
    "meta_base = 'https://api.synopticdata.com/v2/stations/metadata?'\n",
    "api_token = '&token=a2386b75ecbc4c2784db1270695dde73'\n",
    "meta_site = '&stid=%s&complete=1'%site\n",
    "url = meta_base + api_token + meta_site\n",
    "# print(url)\n",
    "\n",
    "site_meta_raw = requests.get(url).json()\n",
    "# print(meta_raw['STATION'][0])\n",
    "\n",
    "zone = site_meta_raw['STATION'][0]['NWSZONE']\n",
    "cwa = site_meta_raw['STATION'][0]['CWA']\n",
    "\n",
    "print('Site: %s\\nCWA: %s\\nZone: %s'%(site, cwa, zone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of sites in the CWA that report precip\n",
    "precip_base = 'https://api.synopticdata.com/v2/stations/precip?&complete=1&interval=6'\n",
    "zone_query = '&nwszone=%s'%zone\n",
    "cwa_query = '&cwa=%s'%cwa\n",
    "date_query = '&start=%s&end=%s'%(\n",
    "    date0.strftime('%Y%m%d%H%M'),\n",
    "    (date0+timedelta(hours=6)).strftime('%Y%m%d%H%M'))\n",
    "\n",
    "# We could query for a list of relevant zones within a CWA here\n",
    "# Then pass a list of zones to the zone query\n",
    "# !Add later!\n",
    "\n",
    "# Fix this later! Temp fix to expand the zone for more NWS/FAA/RAWS stations\n",
    "# Due to 1D Viewer file limitation - Ask Chad for advice?\n",
    "zone_query = cwa_query\n",
    "\n",
    "url = precip_base + api_token + zone_query + date_query\n",
    "zone_meta_raw = requests.get(url).json()\n",
    "\n",
    "meta = []\n",
    "for station in zone_meta_raw['STATION']:\n",
    "    \n",
    "#     We need to get back to a zone query - for now this will work with a CWA query to only hit sites that exist within a CWA    \n",
    "    if (('NWS' in station['SHORTNAME']) | ('RAWS' in station['SHORTNAME']) & (station['STID'][0] == 'K')):\n",
    "        meta.append({k:station[k] for k in station.keys() if type(station[k]) == str})\n",
    "        \n",
    "meta = pd.DataFrame(meta).set_index('STID')\n",
    "\n",
    "meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in meta:\n",
    "    try:\n",
    "        meta[k] = meta[k].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a map of the stations, color by elevation, marker by network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from glob import glob\n",
    "\n",
    "geodir = '../forecast-zones/'\n",
    "zones_shapefile = glob(geodir + '*.shp')[0]\n",
    "\n",
    "# Read the shapefile\n",
    "zones = gpd.read_file(zones_shapefile)\n",
    "# Prune to Western Region using TZ\n",
    "zones = zones.set_index('TIME_ZONE').loc[['M', 'Mm', 'm', 'MP', 'P']].reset_index()\n",
    "zones = zones[zones['CWA'] == cwa]\n",
    "\n",
    "# zones.to_file(geodir + 'forecast-zones.json', driver = 'GeoJSON')\n",
    "zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 16), facecolor='w')\n",
    "ax.set_title('CWA: %s'%cwa)\n",
    "\n",
    "zones.plot(column='NAME', color='0.9', edgecolor='0.25', ax=ax, zorder=10)\n",
    "\n",
    "cbd = ax.scatter(meta['LONGITUDE'], meta['LATITUDE'], c=meta['ELEVATION'], cmap='gist_earth', \n",
    "                 s=150, marker='o', edgecolor='k', linewidth=1.5, zorder=20,)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"3%\", pad=-1.5)\n",
    "plt.colorbar(cbd, cax=cax)\n",
    "ax.grid(True, zorder=-10)\n",
    "\n",
    "ax.set_ylim(bottom=46.30, top=49.10)\n",
    "ax.set_xlim(left=-124.9, right=-120.6)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-process\n",
    "ob_files = [nbm_funcs.get_precip_obs_mp(i) for i in meta.index.values]\n",
    "ob_files = [f for f in ob_files if f is not None]\n",
    "\n",
    "# Multi-process (needs fixing...)\n",
    "# with mp.get_context('fork').Pool(mp.cpu_count()) as p:    \n",
    "#     ob_files = p.map_async(nbm_funcs.get_precip_obs_mp, meta.index.values, chunksize=1)\n",
    "#     ob_files.wait()\n",
    "# ob_files = [f for f in ob_files.get() if f is not None]\n",
    "\n",
    "obs = []\n",
    "for file in ob_files:\n",
    "    site = file.split('/')[-1].split('_')[0]\n",
    "    iobs = pd.read_pickle(file)\n",
    "    iobs['Site'] = np.full(iobs.index.size, fill_value=site, dtype='U10')\n",
    "    iobs = iobs.reset_index().set_index(['ValidTime', 'Site'])\n",
    "    obs.append(iobs)\n",
    "    \n",
    "obs = pd.concat(obs).sort_index()\n",
    "\n",
    "mm_in = 1/25.4\n",
    "obs *= mm_in\n",
    "[obs.rename(columns={k:k.replace('mm', 'in')}, inplace=True) for k in obs.keys()]\n",
    "\n",
    "# OPTIONAL! Drop NaN rows... may help elim lower qual dataw\n",
    "# obs = obs.dropna(how='all')\n",
    "\n",
    "sites = np.unique(obs.index.get_level_values(1))\n",
    "print(sites)\n",
    "\n",
    "print(obs.shape)\n",
    "obs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Obtain NBM forecast data from NBM 1D Viewer (csv file API)\n",
    "These are the NBM 1D output files extracted from the viewer with 3 set accumulation periods<br>\n",
    "See more at: https://hwp-viz.gsd.esrl.noaa.gov/wave1d/?location=KSLC&col=2&hgt=1&obs=true&fontsize=1&selectedgroup=Default\n",
    "<br><br>\n",
    "If no forecast file exists, will download and save for future use. This can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.get_context('fork').Pool(mp.cpu_count()) as p:    \n",
    "    nbm_files = p.map_async(nbm_funcs.get_nbm_1d_mp, meta.index, chunksize=1)\n",
    "    nbm_files.wait()\n",
    "    \n",
    "nbm_files = [f for f in nbm_files.get() if f is not None]\n",
    "\n",
    "nbm = []\n",
    "for file in nbm_files:\n",
    "    site = file.split('/')[-1].split('_')[0]\n",
    "    inbm = pd.read_pickle(file)\n",
    "    inbm['Site'] = np.full(inbm.index.size, fill_value=site, dtype='U10')\n",
    "    inbm = inbm.reset_index().set_index(['InitTime', 'ValidTime', 'Site'])\n",
    "    nbm.append(inbm)\n",
    "    \n",
    "nbm = pd.concat(nbm).sort_index()\n",
    "nbm\n",
    "\n",
    "# Convert mm to in\n",
    "nbm *= mm_in\n",
    "\n",
    "lead = [row[1]-row[0] for row in nbm.index]\n",
    "lead = np.array([1 + row.days*24 + (row.seconds/3600) for row in lead], dtype=int)\n",
    "nbm.insert(0, 'LeadTime', lead)\n",
    "\n",
    "# Nix values where lead time shorter than acc interval\n",
    "for k in [k for k in nbm.keys() if k != 'LeadTime']:\n",
    "    if 'APCP24hr' in k:\n",
    "        nbm[k][nbm['LeadTime'] < 24] = np.nan\n",
    "    elif 'APCP12hr' in k:\n",
    "        nbm[k][nbm['LeadTime'] < 12] = np.nan\n",
    "    elif 'APCP6r' in k:\n",
    "        nbm[k][nbm['LeadTime'] < 6] = np.nan\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "nbm = nbm.dropna(subset=[k for k in nbm.keys() if k != 'LeadTime'], how='all')\n",
    "nbm[25:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some basic stats\n",
    "nbm.loc[:, ['APCP6hr_surface', 'APCP6hr_surface_70% level', 'APCP6hr_surface_50% level',\n",
    "            'APCP12hr_surface', 'APCP12hr_surface_70% level', 'APCP12hr_surface_50% level',\n",
    "            'APCP24hr_surface', 'APCP24hr_surface_70% level', 'APCP24hr_surface_50% level'\n",
    "            ]].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the distribution of precipitation observations vs forecasts for assessment of representativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label fix:\n",
    "site = nbm_funcs._site = zone_query.replace('&', '').replace('=', '_').upper()\n",
    "\n",
    "figdir = nbm_funcs._figdir = '../archive//%s/figures/'%site\n",
    "os.makedirs(figdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_id = nbm_funcs._thresh_id = {'Small':[0, 1], 'Medium':[1, 2], 'Large':[2, 3], 'All':[0, 3]}\n",
    "\n",
    "# 33rd, 67th percentile determined above\n",
    "thresholds = nbm_funcs._thresholds = {interval:nbm_funcs.apcp_dist_plot(obs, nbm, interval, show=True) \n",
    "              for interval in [6, 12, 24]}\n",
    "\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Reorganize the data for analysis:\n",
    "#### Isolate the forecasts by accumulation interval and lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = np.arange(1, 100)\n",
    "\n",
    "matchfile = datadir + 'CWA_%s_%s_%s_verifMatched.NPremoved.pd'%(\n",
    "# matchfile = datadir + 'CWA_%s_%s_%s_verifMatched.pd'%(\n",
    "    cwa, date0.strftime('%Y%m%d'), date1.strftime('%Y%m%d'))\n",
    "\n",
    "if os.path.isfile(matchfile):\n",
    "    data = pd.read_pickle(matchfile)\n",
    "\n",
    "else:\n",
    "    data = []\n",
    "    for interval in [6, 12, 24]:\n",
    "\n",
    "        pkeys = np.array([k for k in nbm.keys() if '%dhr_'%interval in k])\n",
    "        pkeys = np.array([k for k in pkeys if '%' in k])\n",
    "        pkeys = pkeys[np.argsort([int(k.split('_')[-1].split('%')[0]) for k in pkeys])]\n",
    "\n",
    "        for lead_time in np.arange(interval, lead_time_end, 6):\n",
    "\n",
    "            for esize in ['Small', 'Medium', 'Large', 'NP']:\n",
    "\n",
    "                try:\n",
    "                    thresh = [thresholds[interval][thresh_id[esize][0]], \n",
    "                              thresholds[interval][thresh_id[esize][1]]]\n",
    "                except:\n",
    "                    thresh = [0, 0]\n",
    "\n",
    "                print('\\rProcessing interval %d lead %dh'%(interval, lead_time), end='')\n",
    "\n",
    "                # We need to break out the verification to each lead time,\n",
    "                # but within each lead time we have a number of valid times.\n",
    "                # At each lead time, valid time, isolate the forecast verification\n",
    "\n",
    "                # Combine the datasets to make it easier to work with\n",
    "                idata = nbm[nbm['LeadTime'] == lead_time].merge(obs, on=['ValidTime', 'Site']).drop(columns='LeadTime')\n",
    "\n",
    "                # Subset for event size using the observed precip\n",
    "                iobs = idata['%dh_precip_in'%interval]\n",
    "                iobs = iobs.replace(np.nan, 0.) if 'NPremoved' in matchfile else iobs\n",
    "\n",
    "                # Do the trimming of the selected dataset\n",
    "                if esize != 'NP':\n",
    "                    idata = idata[((iobs >= thresh[0]) & (iobs < thresh[1]))]\n",
    "                else:\n",
    "                    idata = idata[iobs == 0]\n",
    "\n",
    "                del iobs\n",
    "\n",
    "                idata['%dh_precip_in'%interval] = (idata['%dh_precip_in'%interval].replace(np.nan, 0) \n",
    "                    if 'NPremoved' in matchfile else idata['%dh_precip_in'%interval])\n",
    "\n",
    "                for itime in idata.index:\n",
    "\n",
    "                    try:\n",
    "                        prob_fx = idata.loc[itime, pkeys]\n",
    "                        mean_fx = np.nanmean(prob_fx)\n",
    "                        std_fx = np.nanstd(prob_fx)\n",
    "                        med_fx = idata.loc[itime, 'APCP%dhr_surface_50%% level'%interval]\n",
    "                        det_fx = idata.loc[itime, 'APCP%dhr_surface'%interval]\n",
    "\n",
    "                        # Optional - leave as nan?\n",
    "                        det_fx = det_fx if ~np.isnan(det_fx) else 0.\n",
    "\n",
    "                        verif_ob = idata.loc[itime, '%dh_precip_in'%interval]\n",
    "\n",
    "                        verif_rank = np.searchsorted(prob_fx, verif_ob, 'right')                    \n",
    "                        verif_rank_val = prob_fx[verif_rank-1]\n",
    "                        verif_rank_error = verif_rank_val - verif_ob\n",
    "\n",
    "                        verif_rank = 101 if ((verif_rank >= 99) & (verif_ob > verif_rank_val)) else verif_rank\n",
    "                        verif_rank = -1 if ((verif_rank <= 1) & (verif_ob < verif_rank_val)) else verif_rank\n",
    "\n",
    "                        det_rank = np.searchsorted(prob_fx, det_fx, 'right')\n",
    "                        det_error = det_fx - verif_ob\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "                        # print('failed', itime)\n",
    "\n",
    "                    else:\n",
    "                        if ~np.isnan(verif_rank_val):\n",
    "\n",
    "                            data.append([\n",
    "                                # Indexers\n",
    "                                interval, lead_time, itime[0], itime[1], esize,\n",
    "\n",
    "                                # Verification and deterministic\n",
    "                                verif_ob, det_fx, det_rank, det_error,\n",
    "\n",
    "                                # Probabilistic\n",
    "                                verif_rank, verif_rank_val, verif_rank_error, \n",
    "                                med_fx, mean_fx, std_fx])\n",
    "\n",
    "    data = pd.DataFrame(data, columns=['Interval', 'LeadTime', 'ValidTime', 'Site', 'EventSize',\n",
    "                    'verif_ob', 'det_fx', 'det_rank', 'det_error',\n",
    "                    'verif_rank', 'verif_rank_val', 'verif_rank_error', \n",
    "                    'med_fx', 'mean_fx', 'std_fx'])\n",
    "\n",
    "    data.to_pickle(matchfile)\n",
    "    \n",
    "print('\\n\\nAvailable keys:\\n\\t\\t{}\\nn rows: {}\\n'.format('\\n\\t\\t'.join(data.keys()), len(data)))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NPEs')\n",
    "\n",
    "fig = plt.figure(facecolor='w', figsize=(10, 6))\n",
    "\n",
    "plt.hist(data[data['EventSize'] == 'NP']['det_fx'], bins=np.arange(0.01, 2.1, .01), edgecolor='k')\n",
    "\n",
    "plt.ylim(top=8000)\n",
    "plt.xlim(left=0)\n",
    "plt.grid()\n",
    "plt.title('CWA: %s\\nNon-Precipitating Forecast Events\\n%s'%(cwa, ''))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "data[((data['EventSize'] == 'NP') & (data['det_fx'] > 0.))][['det_fx']].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Create Bulk Temporal Stats Plots\n",
    "#### Reliability diagrams, bias over time, rank over time, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short, long = 0, 120\n",
    "plot_type = 'Verification'\n",
    "plot_var = 'verif_rank'\n",
    "esize = 'All'\n",
    "\n",
    "for interval in [6, 12, 24]:\n",
    "\n",
    "    kwargs = {'_interval':interval, '_esize':esize,\n",
    "             '_short':short, '_long':long,\n",
    "             '_plot_type':plot_type, '_plot_var':plot_var}\n",
    "    \n",
    "    nbm_funcs.histograms_verif_rank(data, **kwargs, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a reliability diagram style CDF to evaluate percentile rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short, long = 0, 120\n",
    "plot_type = 'Verification'\n",
    "plot_var = 'verif_rank'\n",
    "esize = 'All'\n",
    "\n",
    "for interval in [6, 12, 24]:\n",
    "\n",
    "    kwargs = {'_interval':interval, '_esize':esize,\n",
    "             '_short':short, '_long':long,\n",
    "             '_plot_type':plot_type, '_plot_var':plot_var}\n",
    "\n",
    "    nbm_funcs.reliability_verif_cdf_multistation(data, **kwargs, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce bias, ME, MAE, and percentile rank plots as they evolve over time\n",
    "This helps illustrate at what leads a dry/wet bias may exist and how severe it may be<br>\n",
    "Adds value in interpreting the CDF reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short, long = 0, 120\n",
    "esize = 'All'\n",
    "\n",
    "for interval in [6, 12, 24]:\n",
    "\n",
    "    kwargs = {'_interval':interval, '_esize':esize,\n",
    "             '_short':short, '_long':long}\n",
    "\n",
    "    nbm_funcs.rank_over_leadtime(data, **kwargs, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create maps of rank at each station over the CWA with mean verif and mean det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
