{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in Google CoLab! (Open in new window or new tab)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m-wessler/nbm-verify/blob/master/notebooks/verify_1Dqpf_dev.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import urllib.request as req\n",
    "\n",
    "import scipy.stats as scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nbm_funcs import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Configuration\n",
    "Select 'site' to evaluate, modify 'vsite' if an alternate verification site is preferred<br>\n",
    "Fixed 'date0' at the start of the NBM v3.2 period (2/20/2020)<br>\n",
    "Full lead time is 263 hours - Note if date1 is within this period, there will be missing verification data as it does not exist yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBM 1D Viewer Site to use\n",
    "site = 'KSEA'\n",
    "vsite = site\n",
    "\n",
    "# Data Range\n",
    "lead_time_end = 263\n",
    "init_hours = [1, 7, 13, 19]\n",
    "\n",
    "date0 = datetime(2020, 2, 20)\n",
    "date1 = datetime(2020, 7, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitepath = site if site == vsite else '_'.join([site, vsite])\n",
    "\n",
    "datadir = '../archive/%s/data/'%sitepath\n",
    "os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "figdir = '../archive//%s/figures/'%sitepath\n",
    "os.makedirs(figdir, exist_ok=True)\n",
    "\n",
    "dates = pd.date_range(date0, date1, freq='1D')\n",
    "date2 = date1 + timedelta(hours=lead_time_end)\n",
    "\n",
    "print(('\\nForecast Site: {}\\nVerif Site: {}\\nInit Hours: '+\n",
    "      '{}\\nFirst Init: {}\\nLast Init: {}\\nLast Verif: {}').format(\n",
    "    site, vsite, init_hours, date0, date1, date2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Obtain observation data from SynopticLabs (MesoWest) API\n",
    "These are quality-controlled precipitation observations with adjustable accumulation periods<br>\n",
    "See more at: https://developers.synopticdata.com/mesonet/v2/stations/precipitation/\n",
    "<br><br>\n",
    "If no observation file exists, will download and save for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obfile = datadir + '%s_obs_%s_%s.pd'%(site, date0.strftime('%Y%m%d'), date1.strftime('%Y%m%d'))\n",
    "\n",
    "if os.path.isfile(obfile):\n",
    "    # Load file\n",
    "    obs = pd.read_pickle(obfile)\n",
    "    print('\\nLoaded obs from file %s\\n'%obfile)\n",
    "\n",
    "else:\n",
    "    # Get and save file\n",
    "    obs = get_precip_obs(vsite, date0, date2)\n",
    "    obs = obs[0].merge(obs[1], how='inner', on='ValidTime').merge(obs[2], how='inner', on='ValidTime')\n",
    "    obs = obs[[k for k in obs.keys() if 'precip' in k]].sort_index()\n",
    "\n",
    "    obs.to_pickle(obfile)\n",
    "    print('\\nSaved obs to file %s\\n'%obfile)\n",
    "    \n",
    "mm_in = 1/25.4\n",
    "obs *= mm_in\n",
    "[obs.rename(columns={k:k.replace('mm', 'in')}, inplace=True) for k in obs.keys()]\n",
    "\n",
    "obs.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the distribution of precipitation observations for assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "binsize = 0.05\n",
    "\n",
    "thresholds = {}\n",
    "for interval in [6, 12, 24]:\n",
    "        \n",
    "    iobs = obs['%dh_precip_in'%interval].values\n",
    "    iobs[iobs <= 0.01] = np.nan\n",
    "    \n",
    "    thresholds[interval] = np.nanpercentile(iobs, (33, 67))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6), facecolor='w')\n",
    "    \n",
    "    ax.hist(iobs, bins=np.arange(0, np.nanmax(iobs), binsize), \n",
    "            edgecolor='k', density=True, color='gray', alpha=0.75,\n",
    "            label='PDF (%.2f in bins)'%binsize)\n",
    "    \n",
    "    axx = ax.twinx()\n",
    "    axx.hist(iobs, bins=np.arange(0, np.nanmax(iobs), 0.00001), \n",
    "            density=True, cumulative=True, histtype='step', edgecolor='k', linewidth=2.5)\n",
    "    axx.plot(0, linewidth=2.5, color='k', label='CDF (Continuous)')\n",
    "    \n",
    "    for p, c in zip([33, 67], ['g', 'r']):\n",
    "        ax.axvline(np.nanpercentile(iobs, p), color=c, linewidth=3, zorder=-1, \n",
    "                   label='%dth Percentile: %.2f in'%(p, np.nanpercentile(iobs, p)))\n",
    "\n",
    "    ax.set_xticks(np.arange(0, np.nanmax(iobs)+1, binsize))\n",
    "    ax.set_xticklabels(['%.2f'%v for v in np.arange(0, np.nanmax(iobs)+1, binsize)], rotation=45)\n",
    "    \n",
    "    axx.set_ylabel('\\nCumulative [%]')\n",
    "    axx.set_yticks([0, .2, .4, .6, .8, 1.0])\n",
    "    axx.set_yticklabels([0, 20, 40, 60, 80, 100])\n",
    "    axx.set_ylim([0, 1.01])\n",
    "    \n",
    "    ax.set_xlim([0, np.nanmax(iobs)-0.05])\n",
    "    \n",
    "    ax.set_xlabel('\\n%dh Observed Accumulated Precipitation [in]'%interval)\n",
    "    ax.set_ylabel('Frequency [%]\\n')\n",
    "    ax.set_title('%s\\n%dh Observed Accumulated Precipitation\\nNBM v3.2 Period %s – %s\\n'%(\n",
    "        site, interval, date0.strftime('%Y-%m-%d'), date2.strftime('%Y-%m-%d')))\n",
    "    \n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = axx.get_legend_handles_labels()\n",
    "    axx.legend(lines + lines2, labels + labels2, loc='center right')\n",
    "\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    savestr = '{}_{}h.observedAPCP.png'.format(site, interval)\n",
    "    print(savestr)\n",
    "    \n",
    "    os.makedirs(figdir + 'apcp_dist/', exist_ok=True)\n",
    "    plt.savefig(figdir + 'apcp_dist/' + savestr, dpi=150)\n",
    "    \n",
    "    plt.close()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Obtain NBM forecast data from NBM 1D Viewer (csv file API)\n",
    "These are the NBM 1D output files extracted from the viewer with 3 set accumulation periods<br>\n",
    "See more at: https://hwp-viz.gsd.esrl.noaa.gov/wave1d/?location=KSLC&col=2&hgt=1&obs=true&fontsize=1&selectedgroup=Default\n",
    "<br><br>\n",
    "If no forecast file exists, will download and save for future use. This can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbmfile = datadir + '%s_nbm_%s_%s.pd'%(site, date0.strftime('%Y%m%d'), date1.strftime('%Y%m%d'))\n",
    "\n",
    "if os.path.isfile(nbmfile):\n",
    "    # Load file\n",
    "    nbm = pd.read_pickle(nbmfile)\n",
    "    print('Loaded NBM from file %s'%nbmfile)\n",
    "\n",
    "else:\n",
    "    url_list = []\n",
    "    for date in dates:\n",
    "        for init_hour in init_hours:\n",
    "            # For now pull from the csv generator\n",
    "            # Best to get API access or store locally later\n",
    "            base = 'https://hwp-viz.gsd.esrl.noaa.gov/wave1d/data/archive/'\n",
    "            datestr = '{:04d}/{:02d}/{:02d}'.format(date.year, date.month, date.day)\n",
    "            sitestr = '/NBM/{:02d}/{:s}.csv'.format(init_hour, site)\n",
    "            url_list.append([date, init_hour, base + datestr + sitestr])\n",
    "\n",
    "    # Try multiprocessing this for speed?\n",
    "    nbm = np.array([get_1d_csv(url, this=i+1, total=len(url_list)) for i, url in enumerate(url_list)])\n",
    "    nbm = np.array([line for line in nbm if line is not None])\n",
    "\n",
    "    header = nbm[0, 0]\n",
    "    \n",
    "    # This drops days with incomplete collections. There may be some use\n",
    "    # to keeping this data, can fix in the future if need be\n",
    "    # May also want to make the 100 value flexible!\n",
    "    nbm = np.array([np.array(line[1]) for line in nbm if len(line[1]) == 100])\n",
    "\n",
    "    nbm = nbm.reshape(-1, nbm.shape[-1])\n",
    "    nbm[np.where(nbm == '')] = np.nan\n",
    "\n",
    "    # Aggregate to a clean dataframe\n",
    "    nbm = pd.DataFrame(nbm, columns=header).set_index(\n",
    "        ['InitTime', 'ValidTime']).sort_index()\n",
    "\n",
    "    # Drop last column (misc metadata?)\n",
    "    nbm = nbm.iloc[:, :-2].astype(float)\n",
    "    header = nbm.columns\n",
    "\n",
    "    # variables = np.unique([k.split('_')[0] for k in header])\n",
    "    # levels = np.unique([k.split('_')[1] for k in header])\n",
    "\n",
    "    init =  nbm.index.get_level_values(0)\n",
    "    valid = nbm.index.get_level_values(1)\n",
    "\n",
    "    # Note the 1h 'fudge factor' in the lead time here\n",
    "    lead = pd.DataFrame(\n",
    "        np.transpose([init, valid, ((valid - init).values/3600/1e9).astype(int)+1]), \n",
    "        columns=['InitTime', 'ValidTime', 'LeadTime']).set_index(['InitTime', 'ValidTime'])\n",
    "\n",
    "    nbm.insert(0, 'LeadTime', lead)\n",
    "\n",
    "    klist = np.array([k for k in np.unique([k for k in list(nbm.keys())]) if ('APCP' in k)&('1hr' not in k)])\n",
    "    klist = klist[np.argsort(klist)]\n",
    "    klist = np.append('LeadTime', klist)\n",
    "    nbm = nbm.loc[:, klist]\n",
    "    \n",
    "    # Nix values where lead time shorter than acc interval\n",
    "    for k in nbm.keys():\n",
    "        if 'APCP24hr' in k:\n",
    "            nbm[k][nbm['LeadTime'] < 24] = np.nan\n",
    "        elif 'APCP12hr' in k:\n",
    "            nbm[k][nbm['LeadTime'] < 12] = np.nan\n",
    "        elif 'APCP6hr' in k:\n",
    "            nbm[k][nbm['LeadTime'] < 6] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    nbm.to_pickle(nbmfile)\n",
    "    print('\\nSaved NBM to file %s'%obfile)\n",
    "\n",
    "# Convert mm to in\n",
    "nbm = pd.DataFrame([nbm['LeadTime']] + [nbm[k] * mm_in for k in nbm.keys() if 'LeadTime' not in k]).T\n",
    "\n",
    "# Display some basic stats\n",
    "nbm.loc[:, ['APCP6hr_surface', 'APCP6hr_surface_70% level', 'APCP6hr_surface_50% level',\n",
    "            'APCP12hr_surface', 'APCP12hr_surface_70% level', 'APCP12hr_surface_50% level',\n",
    "            'APCP24hr_surface', 'APCP24hr_surface_70% level', 'APCP24hr_surface_50% level'\n",
    "            ]].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the distribution of precipitation forecasts for assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "binsize = 0.05\n",
    "\n",
    "thresholds = {}\n",
    "for interval in [6, 12, 24]:\n",
    "        \n",
    "    iobs = nbm['APCP%dhr_surface'%interval].values\n",
    "    iobs[iobs <= 0.01] = np.nan\n",
    "    \n",
    "    thresholds[interval] = np.nanpercentile(iobs, (33, 67))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6), facecolor='w')\n",
    "    \n",
    "    ax.hist(iobs, bins=np.arange(0, np.nanmax(iobs), binsize), \n",
    "            edgecolor='k', density=True, color='gray', alpha=0.75,\n",
    "            label='PDF (%.2f in bins)'%binsize)\n",
    "    \n",
    "    axx = ax.twinx()\n",
    "    axx.hist(iobs, bins=np.arange(0, np.nanmax(iobs), 0.00001), \n",
    "            density=True, cumulative=True, histtype='step', edgecolor='k', linewidth=2.5)\n",
    "    axx.plot(0, linewidth=2.5, color='k', label='CDF (Continuous)')\n",
    "    \n",
    "    for p, c in zip([33, 67], ['g', 'r']):\n",
    "        ax.axvline(np.nanpercentile(iobs, p), color=c, linewidth=3, zorder=-1, \n",
    "                   label='%dth Percentile: %.2f in'%(p, np.nanpercentile(iobs, p)))\n",
    "\n",
    "    ax.set_xticks(np.arange(0, np.nanmax(iobs)+1, binsize))\n",
    "    ax.set_xticklabels(['%.2f'%v for v in np.arange(0, np.nanmax(iobs)+1, binsize)], rotation=45)\n",
    "    \n",
    "    axx.set_ylabel('\\nCumulative [%]')\n",
    "    axx.set_yticks([0, .2, .4, .6, .8, 1.0])\n",
    "    axx.set_yticklabels([0, 20, 40, 60, 80, 100])\n",
    "    axx.set_ylim([0, 1.01])\n",
    "    \n",
    "    ax.set_xlim([0, np.nanmax(iobs)-0.05])\n",
    "    \n",
    "    ax.set_xlabel('\\n%dh Forecast Precipitation [in]'%interval)\n",
    "    ax.set_ylabel('Frequency [%]\\n')\n",
    "    ax.set_title('%s\\n%dh Forecast Precipitation\\nNBM v3.2 Period %s – %s\\n'%(\n",
    "        site, interval, date0.strftime('%Y-%m-%d'), date2.strftime('%Y-%m-%d')))\n",
    "    \n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = axx.get_legend_handles_labels()\n",
    "    axx.legend(lines + lines2, labels + labels2, loc='center right')\n",
    "    \n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    savestr = '{}_{}h.detForecastAPCP.png'.format(site, interval)\n",
    "    print(savestr)\n",
    "    \n",
    "    os.makedirs(figdir + 'apcp_dist/', exist_ok=True)\n",
    "    plt.savefig(figdir + 'apcp_dist/' + savestr, dpi=150)\n",
    "    \n",
    "    plt.close()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Reorganize the data for analysis:\n",
    "#### Isolate the forecasts by accumulation interval and lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event size thresholds \n",
    "# Use fixed override if desired else \n",
    "# 33rd, 67th percentile determined above\n",
    "thresholds = {\n",
    "    6:[.1, .25],\n",
    "    12:[.25, .5],\n",
    "    24:[.5, .75]}\n",
    "\n",
    "threshold_sets = [\n",
    "    [0, thresholds[interval][0]], \n",
    "    [thresholds[interval][0], thresholds[interval][1]], \n",
    "    [thresholds[interval][1], np.ceil(np.nanmax(obs))],\n",
    "    [np.nan, np.nan]]\n",
    "\n",
    "plist = np.arange(1, 100)\n",
    "\n",
    "data = []\n",
    "for interval in [6, 12, 24]:\n",
    "    \n",
    "    pkeys = np.array([k for k in nbm.keys() if '%dhr_'%interval in k])\n",
    "    pkeys = np.array([k for k in pkeys if '%' in k])\n",
    "    pkeys = pkeys[np.argsort([int(k.split('_')[-1].split('%')[0]) for k in pkeys])]\n",
    "    \n",
    "    for lead_time in np.arange(interval, lead_time_end, 6):\n",
    "        \n",
    "        for thresh, esize in zip(threshold_sets, ['Small', 'Medium', 'Large']):\n",
    "        \n",
    "            print('\\rProcessing interval %d lead %dh'%(interval, lead_time), end='')\n",
    "\n",
    "            # We need to break out the verification to each lead time,\n",
    "            # but within each lead time we have a number of valid times.\n",
    "            # At each lead time, valid time, isolate the forecast verification\n",
    "\n",
    "            # Combine the datasets to make it easier to work with\n",
    "            idata = nbm[nbm['LeadTime'] == lead_time].merge(obs, on='ValidTime').drop(columns='LeadTime')\n",
    "\n",
    "            # Subset for event size\n",
    "            iobs = idata['%dh_precip_in'%interval]\n",
    "            idata = idata[((iobs >= thresh[0]) & (iobs < thresh[1]))]\n",
    "                \n",
    "            for itime in idata.index:\n",
    "\n",
    "                try:\n",
    "                    prob_fx = idata.loc[itime, pkeys].values\n",
    "                    mean_fx = np.nanmean(prob_fx)\n",
    "                    std_fx = np.nanstd(prob_fx)\n",
    "                    med_fx = idata.loc[itime, 'APCP%dhr_surface_50%% level'%interval]\n",
    "                    det_fx = idata.loc[itime, 'APCP%dhr_surface'%interval]\n",
    "\n",
    "                    # Optional - leave as nan?\n",
    "                    det_fx = det_fx if ~np.isnan(det_fx) else 0.\n",
    "\n",
    "                    verif_ob = idata.loc[itime, '%dh_precip_in'%interval]\n",
    "                    verif_rank = np.searchsorted(prob_fx, verif_ob, 'right')\n",
    "                    verif_rank_val = prob_fx[verif_rank-1]\n",
    "                    verif_rank_error = verif_rank_val - verif_ob\n",
    "\n",
    "                    det_rank = np.searchsorted(prob_fx, det_fx, 'right')\n",
    "                    det_error = det_fx - verif_ob\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "                    # print('failed', itime)\n",
    "\n",
    "                else:\n",
    "                    if ((verif_ob > 0.)):\n",
    "\n",
    "                        data.append([\n",
    "                            # Indexers\n",
    "                            interval, lead_time, itime, esize,\n",
    "\n",
    "                            # Verification and deterministic\n",
    "                            verif_ob, det_fx, det_rank, det_error,\n",
    "\n",
    "                            # Probabilistic\n",
    "                            verif_rank, verif_rank_val, verif_rank_error, \n",
    "                            med_fx, mean_fx, std_fx])\n",
    "\n",
    "data = pd.DataFrame(data, columns=['Interval', 'LeadTime', 'ValidTime', 'EventSize',\n",
    "                'verif_ob', 'det_fx', 'det_rank', 'det_error',\n",
    "                'verif_rank', 'verif_rank_val', 'verif_rank_error', \n",
    "                'med_fx', 'mean_fx', 'std_fx'])\n",
    "\n",
    "print('\\n\\nAvailable keys:\\n\\t\\t{}\\nn rows: {}'.format('\\n\\t\\t'.join(data.keys()), len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Create Bulk Temporal Stats Plots\n",
    "#### Reliability diagrams, bias over time, rank over time, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a pairplot for a basic overview of how certain metrics are correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interval in [6, 12, 24]:\n",
    "    \n",
    "    for short, long in zip([0, 120], [120, 240]):\n",
    "        \n",
    "        for plot_type, plot_var in zip(\n",
    "            ['Verification', 'Comparison'], \n",
    "            ['verif_rank', 'det_rank']):\n",
    "            \n",
    "            for ei, esize in enumerate(['Small', 'Medium', 'Large', 'All']):\n",
    "\n",
    "                select = data[((data['Interval'] == interval)\n",
    "                            & ((data['LeadTime'] >= short) \n",
    "                            & (data['LeadTime'] <= long)))]\n",
    "\n",
    "                select = select[select['EventSize'] == esize] if esize != 'All' else select\n",
    "\n",
    "                try:\n",
    "                    pairplot = sns.pairplot(select, dropna=True, corner=True,\n",
    "                                vars=['verif_rank', 'verif_rank_val', 'verif_rank_error', \n",
    "                                      'det_fx', 'med_fx', 'mean_fx',  'LeadTime', 'verif_ob'])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                else:\n",
    "                    savestr = '{}_{}h_sz{}_lead{}-{}h.pairplot.{}.png'.format(site, interval, esize, short, long, plot_type.lower())\n",
    "                    print(savestr)\n",
    "\n",
    "                    os.makedirs(figdir + 'pairplot/', exist_ok=True)\n",
    "                    plt.savefig(figdir + 'pairplot/' + savestr)\n",
    "                    \n",
    "                    plt.close()\n",
    "                    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a reliability diagram style CDF to evaluate percentile rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interval in [6, 12, 24]:\n",
    "\n",
    "    for short, long in zip([0, 120], [120, 240]):\n",
    "\n",
    "        for plot_type, plot_var in zip(\n",
    "            ['Verification', 'Comparison'], \n",
    "            ['verif_rank', 'det_rank']):\n",
    "\n",
    "            for ei, esize in enumerate(['Small', 'Medium', 'Large', 'All']):\n",
    "                \n",
    "                select = data[((data['Interval'] == interval)\n",
    "                            & ((data['LeadTime'] >= short) \n",
    "                            & (data['LeadTime'] <= long)))]\n",
    "                \n",
    "                select = select[select['EventSize'] == esize] if esize != 'All' else select\n",
    "\n",
    "                # Produce the actual reliability diagram\n",
    "                font_size = 16\n",
    "                plt.rcParams.update({'font.size': font_size})\n",
    "                fig, ax = plt.subplots(1, figsize=(10, 10), facecolor='w')\n",
    "\n",
    "                pbins = np.arange(0, 101, 5)\n",
    "\n",
    "                ax.hist(select[plot_var], bins=pbins, density=True, cumulative=True, histtype='step',\n",
    "                        color='k', edgecolor='k', linewidth=3.5, zorder=10)\n",
    "\n",
    "                ax.hist(select[plot_var], bins=pbins, density=True, cumulative=True,\n",
    "                        color='0.75', linewidth=3.5, zorder=10, label='QPF %s At/Below'%plot_type)\n",
    "\n",
    "                ax.plot(np.arange(0, 101, 1), np.arange(0, 1.01, .01), '--k', linewidth=2, zorder=20)\n",
    "\n",
    "                try:\n",
    "                    ax.axvline(np.nanmean(select[plot_var]), color='g', linewidth=2, \n",
    "                               zorder=200, label='Mean: %d'%np.nanmean(select[plot_var]))\n",
    "\n",
    "                    ax.axvline(np.nanpercentile(select[plot_var], 50), color='r', linewidth=2, \n",
    "                               zorder=200, label='Median: %d'%np.nanpercentile(select[plot_var], 50))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                ax.set_xticks(np.arange(0, 101, 10))\n",
    "                ax.set_xlim([0, 100])\n",
    "                ax.set_xlabel('\\nForecast Percentile')\n",
    "\n",
    "                ax.set_yticks(np.arange(0, 1.01, .1))\n",
    "                ax.set_yticklabels(np.arange(0, 101, 10))\n",
    "                ax.set_ylim([0, 1.01])\n",
    "                ax.set_ylabel('%s Frequency\\n'%plot_type)\n",
    "\n",
    "                ax.set_title(('{} Percentile-Matched {}\\nNBM v3.2 {} – {}\\n\\nEvent Size: {} ({} – {} in)\\n'+\n",
    "                              'Interval: {} h | Lead Time: {} – {} h | n={}\\n').format(\n",
    "                            site, plot_type, date0.strftime('%Y-%m-%d'), date1.strftime('%Y-%m-%d'),\n",
    "                            esize, threshold_sets[ei][0], threshold_sets[ei][1],\n",
    "                            interval, short, long, len(select)), size=font_size)\n",
    "\n",
    "                ax.legend(loc='upper left')\n",
    "                ax.grid()\n",
    "                plt.tight_layout()\n",
    "\n",
    "                savestr = '{}_{}h_sz{}_lead{}-{}h.reliabilityCDF.{}.png'.format(site, interval, esize, short, long, plot_type.lower())\n",
    "                print(savestr)\n",
    "\n",
    "                os.makedirs(figdir + 'reliabilityCDF/', exist_ok=True)\n",
    "                plt.savefig(figdir + 'reliabilityCDF/' + savestr, dpi=150)\n",
    "                \n",
    "                plt.close()\n",
    "                # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce a true reliability diagram along with error histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interval in [6, 12, 24]:\n",
    "\n",
    "    for short, long in zip([0, 120], [120, 240]):\n",
    "\n",
    "        for plot_type, plot_var in zip(\n",
    "            ['Deterministic', 'Median', 'Mean'],\n",
    "            ['det_fx', 'med_fx', 'mean_fx']):\n",
    "\n",
    "            for ei, esize in enumerate(['Small', 'Medium', 'Large', 'All']):\n",
    "                esize = None if esize == 'All' else esize\n",
    "\n",
    "                select = data[((data['Interval'] == interval)\n",
    "                            & ((data['LeadTime'] >= short) \n",
    "                            & (data['LeadTime'] <= long)))]\n",
    "                \n",
    "                select = select[select['EventSize'] == esize] if esize != 'All' else select\n",
    "\n",
    "                try:\n",
    "                    max_val = max(np.nanmax(select['verif_ob']), np.nanmax(select[plot_var]))\n",
    "                    max_val = np.ceil(max_val*10)/10\n",
    "                    max_val = max_val if max_val >= threshold_sets[ei][1] else threshold_sets[ei][1]\n",
    "                \n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                else:\n",
    "                    # Produce the actual reliability diagram\n",
    "                    font_size = 16\n",
    "                    plt.rcParams.update({'font.size': font_size})\n",
    "\n",
    "                    fig, axs = plt.subplots(1, 2, figsize=(20, 10), facecolor='w')\n",
    "\n",
    "                    ax = axs[0]                \n",
    "\n",
    "                    ax.scatter(select['verif_ob'], select[plot_var], c='k', s=150, marker='+', linewidth=0.5, \n",
    "                               label='Forecasts')\n",
    "\n",
    "                    ax.plot(np.arange(0, max_val+.1, .1), np.arange(0, max_val+.1, .1), '--', label='Perfect Forecast')\n",
    "\n",
    "                    \n",
    "                    ax.set_xlim([0, max_val])\n",
    "                    ax.set_xlabel('\\nObserved QPF')\n",
    "\n",
    "                    ax.set_ylim([0, max_val])\n",
    "                    ax.set_ylabel('Forecast QPF\\n')\n",
    "\n",
    "                    ax = axs[1]\n",
    "\n",
    "                    ax.hist(select[plot_var] - select['verif_ob'], bins=np.arange(-0.5, .51, .05), density=True,\n",
    "                            color='0.45', edgecolor='k', linewidth=1, zorder=10, label='Forecast Error')\n",
    "\n",
    "                    ax.axvline(0, c='C0', linestyle='--', linewidth=2.5, zorder=10)\n",
    "                    \n",
    "                    ax.set_xlabel('Forecast Error')\n",
    "                    ax.set_ylabel('Error Frequency [% of Forecasts]')\n",
    "\n",
    "                    for ax in axs:\n",
    "                        ax.legend(loc='upper left')\n",
    "                        ax.grid()\n",
    "\n",
    "                    plt.suptitle(('{} {} Verification\\nNBM v3.2 {} – {}\\n\\nEvent Size: {} ({} – {} in)\\n'+\n",
    "                                  'Interval: {} h | Lead Time: {} – {} h | n={}\\n').format(\n",
    "                                site, plot_type, date0.strftime('%Y-%m-%d'), date1.strftime('%Y-%m-%d'),\n",
    "                                esize, threshold_sets[ei][0], threshold_sets[ei][1],\n",
    "                                interval, short, long, len(select)), size=font_size)\n",
    "\n",
    "                    plt.tight_layout(rect=[0, 0.03, 1, 0.87])\n",
    "\n",
    "                    savestr = '{}_{}h_sz{}_lead{}-{}h.reliability_errhist.{}.png'.format(site, interval, esize, short, long, plot_type.lower())\n",
    "                    print(savestr)\n",
    "                    \n",
    "                    os.makedirs(figdir + 'reliability_errhist/', exist_ok=True)\n",
    "                    plt.savefig(figdir + 'reliability_errhist/' + savestr, dpi=150)\n",
    "                    \n",
    "                    plt.close()\n",
    "                    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce bias, ME, MAE, and percentile rank plots as they evolve over time\n",
    "This helps illustrate at what leads a dry/wet bias may exist and how severe it may be<br>\n",
    "Adds value in interpreting the CDF reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
