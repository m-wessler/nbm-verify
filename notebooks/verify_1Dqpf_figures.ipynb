{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in Google CoLab! (Open in new window or new tab)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m-wessler/nbm-verify/blob/master/notebooks/verify_1Dqpf_dev.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import nbm_funcs\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import seaborn as sns\n",
    "import scipy.stats as scipy\n",
    "import urllib.request as req\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Configuration\n",
    "Select 'site' to evaluate, modify 'vsite' if an alternate verification site is preferred<br>\n",
    "Fixed 'date0' at the start of the NBM v3.2 period (2/20/2020)<br>\n",
    "Full lead time is 263 hours - Note if date1 is within this period, there will be missing verification data as it does not exist yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBM 1D Viewer Site to use\n",
    "# site = sys.argv[1]\n",
    "site = nbm_funcs._site = 'KSEA'\n",
    "vsite = site\n",
    "\n",
    "# Data Range\n",
    "lead_time_end = 263\n",
    "init_hours = [13]#[1, 7, 13, 19]\n",
    "\n",
    "date0 = nbm_funcs._date0 = datetime(2020, 3, 1)\n",
    "date1 = nbm_funcs._date1 = datetime(2020, 7, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forecast Site: KSEA\n",
      "Verif Site: KSEA\n",
      "Init Hours: [13]\n",
      "First Init: 2020-03-01 00:00:00\n",
      "Last Init: 2020-07-15 00:00:00\n",
      "Last Verif: 2020-07-25 23:00:00\n"
     ]
    }
   ],
   "source": [
    "sitepath = site if site == vsite else '_'.join([site, vsite])\n",
    "\n",
    "datadir = nbm_funcs._datadir = '../archive/%s/data/'%sitepath\n",
    "os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "figdir = nbm_funcs._figdir = '../archive//%s/figures/'%sitepath\n",
    "os.makedirs(figdir, exist_ok=True)\n",
    "\n",
    "dates = pd.date_range(date0, date1, freq='1D')\n",
    "date2 = nbm_funcs._date2 = date1 + timedelta(hours=lead_time_end)\n",
    "\n",
    "print(('\\nForecast Site: {}\\nVerif Site: {}\\nInit Hours: '+\n",
    "      '{}\\nFirst Init: {}\\nLast Init: {}\\nLast Verif: {}').format(\n",
    "    site, vsite, init_hours, date0, date1, date2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Obtain observation data from SynopticLabs (MesoWest) API\n",
    "These are quality-controlled precipitation observations with adjustable accumulation periods<br>\n",
    "See more at: https://developers.synopticdata.com/mesonet/v2/stations/precipitation/\n",
    "<br><br>\n",
    "If no observation file exists, will download and save for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded obs from file ../archive/KSEA/data/KSEA_obs_20200301_20200715.pd\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6h_precip_in</th>\n",
       "      <td>558.0</td>\n",
       "      <td>0.017838</td>\n",
       "      <td>0.066073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12h_precip_in</th>\n",
       "      <td>558.0</td>\n",
       "      <td>0.035240</td>\n",
       "      <td>0.107020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24h_precip_in</th>\n",
       "      <td>557.0</td>\n",
       "      <td>0.072631</td>\n",
       "      <td>0.170917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.251969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count      mean       std  min  25%  50%   75%       max\n",
       "6h_precip_in   558.0  0.017838  0.066073  0.0  0.0  0.0  0.00  0.640000\n",
       "12h_precip_in  558.0  0.035240  0.107020  0.0  0.0  0.0  0.01  0.780000\n",
       "24h_precip_in  557.0  0.072631  0.170917  0.0  0.0  0.0  0.05  1.251969"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obfile = datadir + '%s_obs_%s_%s.pd'%(site, date0.strftime('%Y%m%d'), date1.strftime('%Y%m%d'))\n",
    "\n",
    "if os.path.isfile(obfile):\n",
    "    # Load file\n",
    "    obs = pd.read_pickle(obfile)\n",
    "    print('\\nLoaded obs from file %s\\n'%obfile)\n",
    "\n",
    "else:\n",
    "    # Get and save file\n",
    "    obs = get_precip_obs(vsite, date0, date2)\n",
    "    obs = obs[0].merge(obs[1], how='inner', on='ValidTime').merge(obs[2], how='inner', on='ValidTime')\n",
    "    obs = obs[[k for k in obs.keys() if 'precip' in k]].sort_index()\n",
    "\n",
    "    obs.to_pickle(obfile)\n",
    "    print('\\nSaved obs to file %s\\n'%obfile)\n",
    "    \n",
    "mm_in = 1/25.4\n",
    "obs *= mm_in\n",
    "[obs.rename(columns={k:k.replace('mm', 'in')}, inplace=True) for k in obs.keys()]\n",
    "\n",
    "obs.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Obtain NBM forecast data from NBM 1D Viewer (csv file API)\n",
    "These are the NBM 1D output files extracted from the viewer with 3 set accumulation periods<br>\n",
    "See more at: https://hwp-viz.gsd.esrl.noaa.gov/wave1d/?location=KSLC&col=2&hgt=1&obs=true&fontsize=1&selectedgroup=Default\n",
    "<br><br>\n",
    "If no forecast file exists, will download and save for future use. This can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded NBM from file ../archive/KSEA/data/KSEA_nbm_20200301_20200715.pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>APCP6hr_surface</th>\n",
       "      <td>5349.0</td>\n",
       "      <td>0.013527</td>\n",
       "      <td>0.037232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APCP6hr_surface_70% level</th>\n",
       "      <td>5456.0</td>\n",
       "      <td>0.014361</td>\n",
       "      <td>0.037641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006929</td>\n",
       "      <td>0.417087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APCP6hr_surface_50% level</th>\n",
       "      <td>5456.0</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.020444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APCP12hr_surface</th>\n",
       "      <td>5332.0</td>\n",
       "      <td>0.034732</td>\n",
       "      <td>0.075785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.686614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APCP12hr_surface_70% level</th>\n",
       "      <td>5332.0</td>\n",
       "      <td>0.035657</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.040167</td>\n",
       "      <td>0.676181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APCP12hr_surface_50% level</th>\n",
       "      <td>5332.0</td>\n",
       "      <td>0.013262</td>\n",
       "      <td>0.040737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.490551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APCP24hr_surface</th>\n",
       "      <td>2976.0</td>\n",
       "      <td>0.072128</td>\n",
       "      <td>0.127029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.091939</td>\n",
       "      <td>0.789685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APCP24hr_surface_70% level</th>\n",
       "      <td>2976.0</td>\n",
       "      <td>0.085937</td>\n",
       "      <td>0.124312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027559</td>\n",
       "      <td>0.127608</td>\n",
       "      <td>0.813071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APCP24hr_surface_50% level</th>\n",
       "      <td>2976.0</td>\n",
       "      <td>0.040345</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.042293</td>\n",
       "      <td>0.616417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             count      mean       std  min  25%       50%  \\\n",
       "APCP6hr_surface             5349.0  0.013527  0.037232  0.0  0.0  0.000000   \n",
       "APCP6hr_surface_70% level   5456.0  0.014361  0.037641  0.0  0.0  0.000000   \n",
       "APCP6hr_surface_50% level   5456.0  0.004505  0.020444  0.0  0.0  0.000000   \n",
       "APCP12hr_surface            5332.0  0.034732  0.075785  0.0  0.0  0.000000   \n",
       "APCP12hr_surface_70% level  5332.0  0.035657  0.070221  0.0  0.0  0.000394   \n",
       "APCP12hr_surface_50% level  5332.0  0.013262  0.040737  0.0  0.0  0.000000   \n",
       "APCP24hr_surface            2976.0  0.072128  0.127029  0.0  0.0  0.002008   \n",
       "APCP24hr_surface_70% level  2976.0  0.085937  0.124312  0.0  0.0  0.027559   \n",
       "APCP24hr_surface_50% level  2976.0  0.040345  0.081238  0.0  0.0  0.000039   \n",
       "\n",
       "                                 75%       max  \n",
       "APCP6hr_surface             0.000000  0.390000  \n",
       "APCP6hr_surface_70% level   0.006929  0.417087  \n",
       "APCP6hr_surface_50% level   0.000000  0.329173  \n",
       "APCP12hr_surface            0.029331  0.686614  \n",
       "APCP12hr_surface_70% level  0.040167  0.676181  \n",
       "APCP12hr_surface_50% level  0.001024  0.490551  \n",
       "APCP24hr_surface            0.091939  0.789685  \n",
       "APCP24hr_surface_70% level  0.127608  0.813071  \n",
       "APCP24hr_surface_50% level  0.042293  0.616417  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbmfile = datadir + '%s_nbm_%s_%s.pd'%(site, date0.strftime('%Y%m%d'), date1.strftime('%Y%m%d'))\n",
    "\n",
    "if os.path.isfile(nbmfile):\n",
    "    # Load file\n",
    "    nbm = pd.read_pickle(nbmfile)\n",
    "    print('Loaded NBM from file %s'%nbmfile)\n",
    "\n",
    "else:\n",
    "    url_list = []\n",
    "    for date in dates:\n",
    "        for init_hour in init_hours:\n",
    "            # For now pull from the csv generator\n",
    "            # Best to get API access or store locally later\n",
    "            base = 'https://hwp-viz.gsd.esrl.noaa.gov/wave1d/data/archive/'\n",
    "            datestr = '{:04d}/{:02d}/{:02d}'.format(date.year, date.month, date.day)\n",
    "            sitestr = '/NBM/{:02d}/{:s}.csv'.format(init_hour, site)\n",
    "            url_list.append([date, init_hour, base + datestr + sitestr])\n",
    "\n",
    "    # Try multiprocessing this for speed?\n",
    "    nbm = np.array([get_1d_csv(url, this=i+1, total=len(url_list)) for i, url in enumerate(url_list)])\n",
    "    nbm = np.array([line for line in nbm if line is not None])\n",
    "\n",
    "    header = nbm[0, 0]\n",
    "    \n",
    "    # This drops days with incomplete collections. There may be some use\n",
    "    # to keeping this data, can fix in the future if need be\n",
    "    # May also want to make the 100 value flexible!\n",
    "    nbm = np.array([np.array(line[1]) for line in nbm if len(line[1]) == 100])\n",
    "\n",
    "    nbm = nbm.reshape(-1, nbm.shape[-1])\n",
    "    nbm[np.where(nbm == '')] = np.nan\n",
    "\n",
    "    # Aggregate to a clean dataframe\n",
    "    nbm = pd.DataFrame(nbm, columns=header).set_index(\n",
    "        ['InitTime', 'ValidTime']).sort_index()\n",
    "\n",
    "    # Drop last column (misc metadata?)\n",
    "    nbm = nbm.iloc[:, :-2].astype(float)\n",
    "    header = nbm.columns\n",
    "\n",
    "    # variables = np.unique([k.split('_')[0] for k in header])\n",
    "    # levels = np.unique([k.split('_')[1] for k in header])\n",
    "\n",
    "    init =  nbm.index.get_level_values(0)\n",
    "    valid = nbm.index.get_level_values(1)\n",
    "\n",
    "    # Note the 1h 'fudge factor' in the lead time here\n",
    "    lead = pd.DataFrame(\n",
    "        np.transpose([init, valid, ((valid - init).values/3600/1e9).astype(int)+1]), \n",
    "        columns=['InitTime', 'ValidTime', 'LeadTime']).set_index(['InitTime', 'ValidTime'])\n",
    "\n",
    "    nbm.insert(0, 'LeadTime', lead)\n",
    "\n",
    "    klist = np.array([k for k in np.unique([k for k in list(nbm.keys())]) if ('APCP' in k)&('1hr' not in k)])\n",
    "    klist = klist[np.argsort(klist)]\n",
    "    klist = np.append('LeadTime', klist)\n",
    "    nbm = nbm.loc[:, klist]\n",
    "    \n",
    "    # Nix values where lead time shorter than acc interval\n",
    "    for k in nbm.keys():\n",
    "        if 'APCP24hr' in k:\n",
    "            nbm[k][nbm['LeadTime'] < 24] = np.nan\n",
    "        elif 'APCP12hr' in k:\n",
    "            nbm[k][nbm['LeadTime'] < 12] = np.nan\n",
    "        elif 'APCP6hr' in k:\n",
    "            nbm[k][nbm['LeadTime'] < 6] = np.nan\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    nbm.to_pickle(nbmfile)\n",
    "    print('\\nSaved NBM to file %s'%obfile)\n",
    "\n",
    "# Convert mm to in\n",
    "nbm = pd.DataFrame([nbm['LeadTime']] + [nbm[k] * mm_in for k in nbm.keys() if 'LeadTime' not in k]).T\n",
    "\n",
    "# Display some basic stats\n",
    "nbm.loc[:, ['APCP6hr_surface', 'APCP6hr_surface_70% level', 'APCP6hr_surface_50% level',\n",
    "            'APCP12hr_surface', 'APCP12hr_surface_70% level', 'APCP12hr_surface_50% level',\n",
    "            'APCP24hr_surface', 'APCP24hr_surface_70% level', 'APCP24hr_surface_50% level'\n",
    "            ]].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the distribution of precipitation observations vs forecasts for assessment of representativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KSEA_6h.APCP_dist.png\n",
      "KSEA_12h.APCP_dist.png\n",
      "KSEA_24h.APCP_dist.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{6: array([0.        , 0.04      , 0.12183071, 0.64      ]),\n",
       " 12: array([0.        , 0.04711063, 0.15      , 0.78      ]),\n",
       " 24: array([0.        , 0.05103346, 0.2       , 1.2519685 ])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh_id = nbm_funcs._thresh_id = {'Small':[0, 1], 'Medium':[1, 2], 'Large':[2, 3], 'All':[0, 3]}\n",
    "\n",
    "# 33rd, 67th percentile determined above\n",
    "thresholds = nbm_funcs._thresholds = {interval:nbm_funcs.apcp_dist_plot(obs, nbm, interval) \n",
    "              for interval in [6, 12, 24]}\n",
    "\n",
    "# Use fixed override if desired\n",
    "# thresholds = {\n",
    "#     6:[1, 2],\n",
    "#     12:[1, 2],\n",
    "#     24:[1, 2]}\n",
    "\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Reorganize the data for analysis:\n",
    "#### Isolate the forecasts by accumulation interval and lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing interval 24 lead 258h\n",
      "\n",
      "Available keys:\n",
      "\t\tInterval\n",
      "\t\tLeadTime\n",
      "\t\tValidTime\n",
      "\t\tEventSize\n",
      "\t\tverif_ob\n",
      "\t\tdet_fx\n",
      "\t\tdet_rank\n",
      "\t\tdet_error\n",
      "\t\tverif_rank\n",
      "\t\tverif_rank_val\n",
      "\t\tverif_rank_error\n",
      "\t\tmed_fx\n",
      "\t\tmean_fx\n",
      "\t\tstd_fx\n",
      "n rows: 2859\n"
     ]
    }
   ],
   "source": [
    "plist = np.arange(1, 100)\n",
    "\n",
    "data = []\n",
    "for interval in [6, 12, 24]:\n",
    "    \n",
    "    pkeys = np.array([k for k in nbm.keys() if '%dhr_'%interval in k])\n",
    "    pkeys = np.array([k for k in pkeys if '%' in k])\n",
    "    pkeys = pkeys[np.argsort([int(k.split('_')[-1].split('%')[0]) for k in pkeys])]\n",
    "    \n",
    "    for lead_time in np.arange(interval, lead_time_end, 6):\n",
    "        \n",
    "        for esize in ['Small', 'Medium', 'Large']:\n",
    "            \n",
    "            thresh = [thresholds[interval][thresh_id[esize][0]], \n",
    "                      thresholds[interval][thresh_id[esize][1]]]\n",
    "        \n",
    "            print('\\rProcessing interval %d lead %dh'%(interval, lead_time), end='')\n",
    "\n",
    "            # We need to break out the verification to each lead time,\n",
    "            # but within each lead time we have a number of valid times.\n",
    "            # At each lead time, valid time, isolate the forecast verification\n",
    "\n",
    "            # Combine the datasets to make it easier to work with\n",
    "            idata = nbm[nbm['LeadTime'] == lead_time].merge(obs, on='ValidTime').drop(columns='LeadTime')\n",
    "\n",
    "            # Subset for event size\n",
    "            iobs = idata['%dh_precip_in'%interval]\n",
    "            idata = idata[((iobs >= thresh[0]) & (iobs < thresh[1]))]\n",
    "\n",
    "            for itime in idata.index:\n",
    "\n",
    "                try:\n",
    "                    prob_fx = idata.loc[itime, pkeys].values\n",
    "                    mean_fx = np.nanmean(prob_fx)\n",
    "                    std_fx = np.nanstd(prob_fx)\n",
    "                    med_fx = idata.loc[itime, 'APCP%dhr_surface_50%% level'%interval]\n",
    "                    det_fx = idata.loc[itime, 'APCP%dhr_surface'%interval]\n",
    "\n",
    "                    # Optional - leave as nan?\n",
    "                    det_fx = det_fx if ~np.isnan(det_fx) else 0.\n",
    "\n",
    "                    verif_ob = idata.loc[itime, '%dh_precip_in'%interval]\n",
    "                    \n",
    "                    verif_rank = np.searchsorted(prob_fx, verif_ob, 'right')                    \n",
    "                    verif_rank_val = prob_fx[verif_rank-1]\n",
    "                    verif_rank_error = verif_rank_val - verif_ob\n",
    "                    \n",
    "                    verif_rank = 101 if ((verif_rank >= 99) & (verif_ob > verif_rank_val)) else verif_rank\n",
    "                    verif_rank = -1 if ((verif_rank <= 1) & (verif_ob < verif_rank_val)) else verif_rank\n",
    "                    \n",
    "                    det_rank = np.searchsorted(prob_fx, det_fx, 'right')\n",
    "                    det_error = det_fx - verif_ob\n",
    "\n",
    "                except:\n",
    "                    raise\n",
    "                    # pass\n",
    "                    # print('failed', itime)\n",
    "\n",
    "                else:\n",
    "                    if ((verif_ob > 0.) & ~np.isnan(verif_rank_val)):\n",
    "\n",
    "                        data.append([\n",
    "                            # Indexers\n",
    "                            interval, lead_time, itime, esize,\n",
    "\n",
    "                            # Verification and deterministic\n",
    "                            verif_ob, det_fx, det_rank, det_error,\n",
    "\n",
    "                            # Probabilistic\n",
    "                            verif_rank, verif_rank_val, verif_rank_error, \n",
    "                            med_fx, mean_fx, std_fx])\n",
    "\n",
    "data = pd.DataFrame(data, columns=['Interval', 'LeadTime', 'ValidTime', 'EventSize',\n",
    "                'verif_ob', 'det_fx', 'det_rank', 'det_error',\n",
    "                'verif_rank', 'verif_rank_val', 'verif_rank_error', \n",
    "                'med_fx', 'mean_fx', 'std_fx'])\n",
    "\n",
    "print('\\n\\nAvailable keys:\\n\\t\\t{}\\nn rows: {}'.format('\\n\\t\\t'.join(data.keys()), len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Create Bulk Temporal Stats Plots\n",
    "#### Reliability diagrams, bias over time, rank over time, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot histograms of percentile rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 79\n",
      "KSEA_6h_szAll_lead0-120h.rankPDF.verification.png\n",
      "All 123\n",
      "KSEA_12h_szAll_lead0-120h.rankPDF.verification.png\n",
      "All 145\n",
      "KSEA_24h_szAll_lead0-120h.rankPDF.verification.png\n"
     ]
    }
   ],
   "source": [
    "short, long = 0, 120\n",
    "plot_type = 'Verification'\n",
    "plot_var = 'verif_rank'\n",
    "esize = 'All'\n",
    "\n",
    "for interval in [6, 12, 24]:\n",
    "\n",
    "    kwargs = {'_interval':interval, '_esize':esize,\n",
    "             '_short':short, '_long':long,\n",
    "             '_plot_type':plot_type, '_plot_var':plot_var}\n",
    "    \n",
    "    nbm_funcs.histograms_verif_rank(data, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a reliability diagram style CDF to evaluate percentile rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interval in [6, 12, 24]:\n",
    "    short, long = 0, 120\n",
    "    plot_type = 'Verification'\n",
    "    plot_var = 'verif_rank'\n",
    "    \n",
    "    ei, esize = 3, 'All'\n",
    "\n",
    "    select = data[((data['Interval'] == interval)\n",
    "                & ((data['LeadTime'] >= short) \n",
    "                & (data['LeadTime'] <= long)))]\n",
    "\n",
    "    select = select[select['EventSize'] == esize] if esize != 'All' else select\n",
    "\n",
    "    # Produce the actual reliability diagram\n",
    "    font_size = 16\n",
    "    plt.rcParams.update({'font.size': font_size})\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10), facecolor='w')\n",
    "\n",
    "    pbinsize = 5\n",
    "    pbins = np.arange(0, 101, pbinsize)\n",
    "\n",
    "    hist = ax.hist(select[plot_var], bins=pbins, density=True, cumulative=True,\n",
    "            color='w', alpha=0, linewidth=3.5, zorder=10)\n",
    "    histy, histx = hist[0]*100, hist[1][1:]-(pbinsize/2)\n",
    "\n",
    "    ax.plot(histx, histy, marker='x', linestyle='--', markersize=10, color='k', linewidth=2)#, label='QPF Rank %s'%plot_type)\n",
    "\n",
    "    ax.plot(np.arange(0, 101, 1), np.arange(0, 101, 1), '--k', linewidth=1, zorder=20)\n",
    "    # ax.plot(np.arange(0, 1.01, .01), np.arange(0, 1.01, .01), '--k', linewidth=1, zorder=20)\n",
    "\n",
    "    # try:\n",
    "    #     ax.axvline(np.nanmean(select[plot_var]), color=colors[0], linewidth=3, \n",
    "    #                zorder=-1, label='Mean: %d'%np.nanmean(select[plot_var]))\n",
    "\n",
    "    #     ax.axvline(np.nanpercentile(select[plot_var], 50), color=colors[1], linewidth=3, \n",
    "    #                zorder=-1, label='Median: %d'%np.nanpercentile(select[plot_var], 50))\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    ax.set_xticks(np.arange(0, 101, 5))\n",
    "    ax.set_xlim([0, 101])\n",
    "\n",
    "    ax.set_yticks(np.arange(0, 101, 5))\n",
    "    ax.set_yticklabels(np.arange(0, 101, 5), rotation=30)\n",
    "    ax.set_ylim([0, 101])\n",
    "\n",
    "    ax.set_yticklabels(np.arange(100, -5, -5), rotation=30)\n",
    "\n",
    "    ax.set_ylabel('\\nObserved in % of Forecasts')\n",
    "    ax.set_xlabel('\\nForecast Verifies At/Above Percentile\\n')\n",
    "\n",
    "    n_precip_periods = np.unique(select['ValidTime'][~np.isnan(select['verif_ob'])]).shape[0]\n",
    "    ax.set_title(('{} Percentile-Matched {}\\nNBM v3.2 {} – {}\\n\\nEvent Size: {}\\n'+#' ({} – {} in)\\n'+\n",
    "                  'Interval: {} h | Lead Time: {} – {} h\\nn={}, np={}\\n').format(\n",
    "                site, plot_type, date0.strftime('%Y-%m-%d'), date1.strftime('%Y-%m-%d'),\n",
    "                esize, #threshold_sets[ei][0], threshold_sets[ei][1],\n",
    "                interval, short, long, len(select), n_precip_periods), size=font_size)\n",
    "\n",
    "    ax.text(5, 92, 'Wet Bias')\n",
    "    ax.text(85, 6, 'Dry Bias')\n",
    "    ax.text(35, 38, 'Unbiased Distribution', rotation=40)\n",
    "\n",
    "    # ax.legend(loc='upper left')\n",
    "    ax.grid()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    savestr = '{}_{}h_sz{}_lead{}-{}h.reliabilityCDF.{}.png'.format(site, interval, esize, short, long, plot_type.lower())\n",
    "    print(savestr)\n",
    "\n",
    "    os.makedirs(figdir + 'reliabilityCDF/', exist_ok=True)\n",
    "    plt.savefig(figdir + 'reliabilityCDF/' + savestr, dpi=150)\n",
    "\n",
    "    # plt.close()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce a true reliability diagram along with error histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for interval in [6, 12, 24]:\n",
    "\n",
    "#     for short, long in zip([0, 120], [120, 240]):\n",
    "\n",
    "#         for plot_type, plot_var in zip(\n",
    "#             ['Deterministic', 'Median', 'Mean'],\n",
    "#             ['det_fx', 'med_fx', 'mean_fx']):\n",
    "\n",
    "#             for ei, esize in enumerate(['Small', 'Medium', 'Large', 'All']):\n",
    "#                 esize = None if esize == 'All' else esize\n",
    "\n",
    "#                 select = data[((data['Interval'] == interval)\n",
    "#                             & ((data['LeadTime'] >= short) \n",
    "#                             & (data['LeadTime'] <= long)))]\n",
    "                \n",
    "#                 select = select[select['EventSize'] == esize] if esize != 'All' else select\n",
    "\n",
    "#                 try:\n",
    "#                     max_val = max(np.nanmax(select['verif_ob']), np.nanmax(select[plot_var]))\n",
    "#                     max_val = np.ceil(max_val*10)/10\n",
    "#                     max_val = max_val if max_val >= threshold_sets[ei][1] else threshold_sets[ei][1]\n",
    "                \n",
    "#                 except:\n",
    "#                     pass\n",
    "                \n",
    "#                 else:\n",
    "#                     # Produce the actual reliability diagram\n",
    "#                     font_size = 16\n",
    "#                     plt.rcParams.update({'font.size': font_size})\n",
    "\n",
    "#                     fig, axs = plt.subplots(1, 2, figsize=(20, 10), facecolor='w')\n",
    "\n",
    "#                     ax = axs[0]                \n",
    "\n",
    "#                     ax.scatter(select['verif_ob'], select[plot_var], c='k', s=150, marker='+', linewidth=0.5, \n",
    "#                                label='Forecasts')\n",
    "\n",
    "#                     ax.plot(np.arange(0, max_val+.1, .1), np.arange(0, max_val+.1, .1), '--', label='Perfect Forecast')\n",
    "\n",
    "                    \n",
    "#                     ax.set_xlim([0, max_val])\n",
    "#                     ax.set_xlabel('\\nObserved QPF')\n",
    "\n",
    "#                     ax.set_ylim([0, max_val])\n",
    "#                     ax.set_ylabel('Forecast QPF\\n')\n",
    "\n",
    "#                     ax = axs[1]\n",
    "\n",
    "#                     ax.hist(select[plot_var] - select['verif_ob'], bins=np.arange(-0.5, .51, .05), density=True,\n",
    "#                             color='0.45', edgecolor='k', linewidth=1, zorder=10, label='Forecast Error')\n",
    "\n",
    "#                     ax.axvline(0, c='C0', linestyle='--', linewidth=2.5, zorder=10)\n",
    "                    \n",
    "#                     ax.set_xlabel('Forecast Error')\n",
    "#                     ax.set_ylabel('Error Frequency [% of Forecasts]')\n",
    "\n",
    "#                     for ax in axs:\n",
    "#                         ax.legend(loc='upper left')\n",
    "#                         ax.grid()\n",
    "\n",
    "#                     plt.suptitle(('{} {} Verification\\nNBM v3.2 {} – {}\\n\\nEvent Size: {} ({} – {} in)\\n'+\n",
    "#                                   'Interval: {} h | Lead Time: {} – {} h | n={}\\n').format(\n",
    "#                                 site, plot_type, date0.strftime('%Y-%m-%d'), date1.strftime('%Y-%m-%d'),\n",
    "#                                 esize, threshold_sets[ei][0], threshold_sets[ei][1],\n",
    "#                                 interval, short, long, len(select)), size=font_size)\n",
    "\n",
    "#                     plt.tight_layout(rect=[0, 0.03, 1, 0.87])\n",
    "\n",
    "#                     savestr = '{}_{}h_sz{}_lead{}-{}h.reliability_errhist.{}.png'.format(site, interval, esize, short, long, plot_type.lower())\n",
    "#                     print(savestr)\n",
    "                    \n",
    "#                     os.makedirs(figdir + 'reliability_errhist/', exist_ok=True)\n",
    "#                     plt.savefig(figdir + 'reliability_errhist/' + savestr, dpi=150)\n",
    "                    \n",
    "#                     plt.close()\n",
    "#                     # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce bias, ME, MAE, and percentile rank plots as they evolve over time\n",
    "This helps illustrate at what leads a dry/wet bias may exist and how severe it may be<br>\n",
    "Adds value in interpreting the CDF reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interval in [6, 12, 24]:\n",
    "    \n",
    "    short, long = 0, 120\n",
    "    plot_type = 'Verification'\n",
    "    plot_var = 'verif_rank'\n",
    "    ei, esize = 3, 'All'\n",
    "                \n",
    "    select = data[((data['Interval'] == interval)\n",
    "                & ((data['LeadTime'] >= short) \n",
    "                & (data['LeadTime'] <= long)))]\n",
    "\n",
    "    select = select[select['EventSize'] == esize] if esize != 'All' else select\n",
    "\n",
    "    t0, t1 = [threshold_sets[ei][0], threshold_sets[ei][1]] if esize != 'All' else [0, round(np.nanmax(select['verif_ob']), 2)]\n",
    "\n",
    "    plt.rcParams.update({'font.size': 13})\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 8), facecolor='w')\n",
    "    ax3, ax1, ax2 = axs.flatten()\n",
    "    ms, lw = 50, 1\n",
    "\n",
    "    for i, lead_time in enumerate(np.unique(select['LeadTime'])):\n",
    "\n",
    "        lead_data = select[select['LeadTime'] == lead_time]\n",
    "        lead_data = lead_data.dropna()\n",
    "\n",
    "        # Lead time vs Mean Rank Verification\n",
    "        label = 'Mean Verification Rank' if i == 0 else None\n",
    "        ax1.scatter(lead_time, np.nanmean(lead_data['verif_rank']), c='g', marker='_', s=ms*5, linewidth=lw*2, label=label)\n",
    "\n",
    "        label = 'Median Verification Rank' if i == 0 else None\n",
    "        ax1.scatter(lead_time, np.nanpercentile(lead_data['verif_rank'], 50), c='r',marker='_', s=ms*5, linewidth=lw*2, label=label)\n",
    "\n",
    "        # Lead time vs Mean Deterministic Comparason\n",
    "        label = 'Mean Deterministic Rank' if i == 0 else None\n",
    "        ax2.scatter(lead_time, np.nanmean(lead_data['det_rank']), c='g', marker='_', s=ms*5, linewidth=lw*2, label=label)\n",
    "\n",
    "        label = 'Median Deterministic Rank' if i == 0 else None\n",
    "        ax2.scatter(lead_time, np.nanpercentile(lead_data['det_rank'], 50), c='r', marker='_', s=ms*5, linewidth=lw*2, label=label)\n",
    "\n",
    "        ax3.scatter(lead_time, np.nanmean(lead_data['det_fx'] - lead_data['verif_ob']), c='k', marker='_', s=ms*5, linewidth=lw*2)\n",
    "        # ax3.scatter(lead_time, np.nanmean(lead_data['verif_rank_val'] - lead_data['verif_ob']), c='C0', marker='_', s=ms*5, linewidth=lw*2)\n",
    "        ax3.scatter(lead_time, np.nanmean(lead_data['mean_fx'] - lead_data['verif_ob']), c='g', marker='_', s=ms*5, linewidth=lw*2)\n",
    "        ax3.scatter(lead_time, np.nanmean(lead_data['med_fx'] - lead_data['verif_ob']), c='r', marker='_', s=ms*5, linewidth=lw*2)\n",
    "\n",
    "    ax3.scatter(-10, 0, c='k', marker='_', s=ms*5, linewidth=lw*2, label='Deterministic')\n",
    "    # ax3.scatter(-10, 0, c='C0', marker='_', s=ms*5, linewidth=lw*2, label='Rank-Matched')\n",
    "    ax3.scatter(-10, 0, c='g', marker='_', s=ms*5, linewidth=lw*2, label='Mean')\n",
    "    ax3.scatter(-10, 0, c='r', marker='_', s=ms*5, linewidth=lw*2, label='Median')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xticks(np.arange(short, long+1, 12))\n",
    "        ax.set_xlim([short-6, long+2.5])\n",
    "        ax.set_xticklabels(np.arange(short, long+1, 12), rotation=45)\n",
    "        ax.set_yticks(np.arange(0, 101, 10))\n",
    "        ax.legend(loc='lower right', fontsize='small')\n",
    "        ax.grid()\n",
    "        ax.set_xlim([short, long+2.5])\n",
    "\n",
    "    ax3.set_ylabel('Mean Error (in)\\n(Forecast - Observation)\\n')\n",
    "\n",
    "    for ax in [ax3]:\n",
    "        ax.axhline(0, color='k', linestyle='--', linewidth=1.5)\n",
    "        ax.set_yticks(np.arange(-.5, .251, .05))\n",
    "        ax.set_ylim([-.5, .25])\n",
    "        ax.legend(loc='lower left', fontsize='small')\n",
    "        ax.set_xlabel('Lead Time')\n",
    "\n",
    "    plt.suptitle(('{} Verification\\nNBM v3.2 {} – {}\\n\\nEvent Size: {} ({} – {} in)\\n'+\n",
    "                  'Interval: {} h | Lead Time: {} – {} h | n={}\\n').format(\n",
    "                site, date0.strftime('%Y-%m-%d'), date1.strftime('%Y-%m-%d'),\n",
    "                esize, t0, t1,\n",
    "                interval, short, long, len(select)), size=font_size)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.85])\n",
    "\n",
    "    savestr = '{}_{}h_sz{}_lead{}-{}h.rank_err_overtime.png'.format(site, interval, esize, short, long)\n",
    "    print(savestr)\n",
    "\n",
    "    os.makedirs(figdir + 'rank_err_overtime/', exist_ok=True)\n",
    "    plt.savefig(figdir + 'rank_err_overtime/' + savestr, dpi=150)\n",
    "\n",
    "    #plt.close()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
